{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":104491,"databundleVersionId":12585144,"sourceType":"competition"},{"sourceId":12139972,"sourceType":"datasetVersion","datasetId":7645561}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Land Cover Classification using NDVI Time-Series Data\n\nThis notebook implements a solution for the Summer Analytics 2025 Hackathon challenge to classify land cover types using NDVI time-series data from satellite imagery.","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom scipy import stats\nfrom sklearn.utils import class_weight","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:12:44.397733Z","iopub.execute_input":"2025-06-12T08:12:44.398176Z","iopub.status.idle":"2025-06-12T08:12:44.403812Z","shell.execute_reply.started":"2025-06-12T08:12:44.398146Z","shell.execute_reply":"2025-06-12T08:12:44.402813Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Data Loading and Exploration","metadata":{}},{"cell_type":"code","source":"# Load the data\ntraindf = pd.read_csv(\"/kaggle/input/summer-analytics-mid-hackathon/hacktrain.csv\")\ntestdf = pd.read_csv(\"/kaggle/input/summer-analytics-mid-hackathon/hacktest.csv\")\n\n# Display basic information\nprint(\"Training data shape:\", traindf.shape)\nprint(\"Test data shape:\", testdf.shape)\n\n# Display first few rows of training data\ntraindf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:12:56.892900Z","iopub.execute_input":"2025-06-12T08:12:56.893349Z","iopub.status.idle":"2025-06-12T08:12:56.964951Z","shell.execute_reply.started":"2025-06-12T08:12:56.893315Z","shell.execute_reply":"2025-06-12T08:12:56.964268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for missing values\nprint(\"Missing values in training data:\")\nprint(traindf.isnull().sum())\n\n# Check class distribution\nprint(\"\\nClass distribution:\")\nclass_counts = traindf['class'].value_counts()\nprint(class_counts)\n\n# Visualize class distribution\nplt.figure(figsize=(10, 6))\nclass_counts.plot(kind='bar')\nplt.title('Land Cover Class Distribution')\nplt.xlabel('Land Cover Type')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:12:57.872486Z","iopub.execute_input":"2025-06-12T08:12:57.872794Z","iopub.status.idle":"2025-06-12T08:12:58.338276Z","shell.execute_reply.started":"2025-06-12T08:12:57.872771Z","shell.execute_reply":"2025-06-12T08:12:58.337320Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Data Preprocessing and Feature Engineering","metadata":{}},{"cell_type":"code","source":"# Drop unnecessary columns\ntraindf.drop(['Unnamed: 0'], axis=1, inplace=True)\ntestdf.drop(['Unnamed: 0'], axis=1, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:13:02.173007Z","iopub.execute_input":"2025-06-12T08:13:02.173875Z","iopub.status.idle":"2025-06-12T08:13:02.182671Z","shell.execute_reply.started":"2025-06-12T08:13:02.173837Z","shell.execute_reply":"2025-06-12T08:13:02.181827Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Feature engineering - create statistical features from NDVI time series\ndef create_features(df):\n    # Get only NDVI columns\n    ndvi_cols = [col for col in df.columns if '_N' in col]\n    \n    # Create a copy to avoid modifying the original dataframe\n    df_features = df.copy()\n    \n    # Calculate statistical features\n    df_features['ndvi_mean'] = df[ndvi_cols].mean(axis=1)\n    df_features['ndvi_std'] = df[ndvi_cols].std(axis=1)\n    df_features['ndvi_min'] = df[ndvi_cols].min(axis=1)\n    df_features['ndvi_max'] = df[ndvi_cols].max(axis=1)\n    df_features['ndvi_range'] = df_features['ndvi_max'] - df_features['ndvi_min']\n    \n    # Calculate first and second half differences (temporal change)\n    half_point = len(ndvi_cols) // 2\n    first_half = ndvi_cols[:half_point]\n    second_half = ndvi_cols[half_point:]\n    \n    df_features['first_half_mean'] = df[first_half].mean(axis=1)\n    df_features['second_half_mean'] = df[second_half].mean(axis=1)\n    df_features['half_diff'] = df_features['second_half_mean'] - df_features['first_half_mean']\n    \n    # Calculate quarterly statistics if we have enough data points\n    if len(ndvi_cols) >= 4:\n        quarter_size = len(ndvi_cols) // 4\n        q1 = ndvi_cols[:quarter_size]\n        q2 = ndvi_cols[quarter_size:2*quarter_size]\n        q3 = ndvi_cols[2*quarter_size:3*quarter_size]\n        q4 = ndvi_cols[3*quarter_size:]\n        \n        if q1:\n            df_features['q1_mean'] = df[q1].mean(axis=1)\n        if q2:\n            df_features['q2_mean'] = df[q2].mean(axis=1)\n        if q3:\n            df_features['q3_mean'] = df[q3].mean(axis=1)\n        if q4:\n            df_features['q4_mean'] = df[q4].mean(axis=1)\n    \n    return df_features\n\n# Apply feature engineering\ntrain_features = create_features(traindf)\ntest_features = create_features(testdf)\n\n# Display the new features\nprint(\"New features added:\")\nnew_features = [col for col in train_features.columns if col not in traindf.columns]\nprint(new_features)\n\n# Handle missing values with median imputation\ntrain_features.fillna(train_features.median(numeric_only=True), inplace=True)\ntest_features.fillna(test_features.median(numeric_only=True), inplace=True)\n\n# Verify no missing values remain\nprint(\"\\nMissing values after imputation:\")\nprint(train_features.isnull().sum().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:13:03.002991Z","iopub.execute_input":"2025-06-12T08:13:03.003346Z","iopub.status.idle":"2025-06-12T08:13:03.109719Z","shell.execute_reply.started":"2025-06-12T08:13:03.003321Z","shell.execute_reply":"2025-06-12T08:13:03.108906Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Visualize NDVI Patterns by Land Cover Class","metadata":{}},{"cell_type":"code","source":"# Get NDVI columns\nndvi_cols = [col for col in traindf.columns if '_N' in col]\n\n# Visualize average NDVI time series for each class\nplt.figure(figsize=(15, 10))\nfor i, class_name in enumerate(traindf['class'].unique()):\n    class_data = traindf[traindf['class'] == class_name][ndvi_cols].mean()\n    plt.plot(range(len(ndvi_cols)), class_data, label=class_name, linewidth=2)\n\nplt.title('Average NDVI Time Series by Land Cover Class')\nplt.xlabel('Time Point')\nplt.ylabel('NDVI Value')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:13:03.672538Z","iopub.execute_input":"2025-06-12T08:13:03.672866Z","iopub.status.idle":"2025-06-12T08:13:04.142241Z","shell.execute_reply.started":"2025-06-12T08:13:03.672839Z","shell.execute_reply":"2025-06-12T08:13:04.141300Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Model Building with Logistic Regression","metadata":{}},{"cell_type":"code","source":"# Prepare data for modeling\nX = train_features.drop(columns=['class', 'ID'])\ny = train_features['class']\n\n# Split data\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)\n\nprint(f\"Training set: {X_train.shape[0]} samples\")\nprint(f\"Validation set: {X_val.shape[0]} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:13:04.232309Z","iopub.execute_input":"2025-06-12T08:13:04.232643Z","iopub.status.idle":"2025-06-12T08:13:04.248490Z","shell.execute_reply.started":"2025-06-12T08:13:04.232619Z","shell.execute_reply":"2025-06-12T08:13:04.247398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a pipeline with preprocessing and model\npipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler()),\n    ('classifier', LogisticRegression(max_iter=1000, multi_class='multinomial', solver='newton-cg'))\n])\n\n# Define parameters for grid search\nparam_grid = {\n    'classifier__C': [1, 1.5, 2, 3, 4],\n    'classifier__tol': [0.0001, 0.00001],\n    'classifier__class_weight': [None, 'balanced']\n}\n\n# Create grid search\ngrid_search = GridSearchCV(\n    pipeline,\n    param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=1\n)\n\n# Fit the model\ngrid_search.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:13:04.528131Z","iopub.execute_input":"2025-06-12T08:13:04.528542Z","iopub.status.idle":"2025-06-12T08:13:28.533791Z","shell.execute_reply.started":"2025-06-12T08:13:04.528516Z","shell.execute_reply":"2025-06-12T08:13:28.532802Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print best parameters\nprint(\"Best parameters:\", grid_search.best_params_)\nprint(\"Best cross-validation score:\", grid_search.best_score_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:13:28.535260Z","iopub.execute_input":"2025-06-12T08:13:28.535551Z","iopub.status.idle":"2025-06-12T08:13:28.540534Z","shell.execute_reply.started":"2025-06-12T08:13:28.535531Z","shell.execute_reply":"2025-06-12T08:13:28.539497Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Model Evaluation","metadata":{}},{"cell_type":"code","source":"# Evaluate on validation set\ny_pred = grid_search.predict(X_val)\nprint(\"Classification Report on Validation Set:\")\nprint(classification_report(y_val, y_pred))\n\n# Calculate accuracy\naccuracy = accuracy_score(y_val, y_pred)\nprint(f\"Validation Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n\n# Plot confusion matrix\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(y_val, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=np.unique(y), \n            yticklabels=np.unique(y))\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:13:28.541552Z","iopub.execute_input":"2025-06-12T08:13:28.541897Z","iopub.status.idle":"2025-06-12T08:13:28.998272Z","shell.execute_reply.started":"2025-06-12T08:13:28.541868Z","shell.execute_reply":"2025-06-12T08:13:28.997300Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Feature Importance Analysis","metadata":{}},{"cell_type":"code","source":"# Analyze feature importance\nbest_model = grid_search.best_estimator_.named_steps['classifier']\nfeature_names = X.columns\ncoefficients = best_model.coef_\nclasses = best_model.classes_\n\n# Plot feature importance for each class\nplt.figure(figsize=(15, 15))\nfor i, class_name in enumerate(classes):\n    plt.subplot(3, 2, i+1)\n    sorted_idx = np.argsort(np.abs(coefficients[i]))\n    top_features = sorted_idx[-10:]\n    plt.barh(np.array(feature_names)[top_features], coefficients[i][top_features])\n    plt.title(f'Top 10 Features for {class_name}')\n    plt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:13:28.999864Z","iopub.execute_input":"2025-06-12T08:13:29.000191Z","iopub.status.idle":"2025-06-12T08:13:30.487048Z","shell.execute_reply.started":"2025-06-12T08:13:29.000170Z","shell.execute_reply":"2025-06-12T08:13:30.485977Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Final Model and Predictions","metadata":{}},{"cell_type":"code","source":"# Train final model on all training data\nfinal_model = grid_search.best_estimator_\nfinal_model.fit(X, y)\n\n# Prepare test data\nX_test = test_features.drop(columns=['ID'])\n\n# Make predictions\ntest_predictions = final_model.predict(X_test)\n\n# Display prediction distribution\nprint(\"Prediction distribution:\")\npred_counts = pd.Series(test_predictions).value_counts()\nprint(pred_counts)\n\n# Visualize prediction distribution\nplt.figure(figsize=(10, 6))\npred_counts.plot(kind='bar')\nplt.title('Predicted Land Cover Class Distribution')\nplt.xlabel('Land Cover Type')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:13:49.613063Z","iopub.execute_input":"2025-06-12T08:13:49.613766Z","iopub.status.idle":"2025-06-12T08:13:50.450882Z","shell.execute_reply.started":"2025-06-12T08:13:49.613740Z","shell.execute_reply":"2025-06-12T08:13:50.449933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create submission file\nsubmission = pd.DataFrame({\n    'ID': testdf['ID'],\n    'class': test_predictions\n})\n\n# Display first few rows of submission\nsubmission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:14:03.652902Z","iopub.execute_input":"2025-06-12T08:14:03.653303Z","iopub.status.idle":"2025-06-12T08:14:03.664262Z","shell.execute_reply.started":"2025-06-12T08:14:03.653280Z","shell.execute_reply":"2025-06-12T08:14:03.663150Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save submission\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file created: submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:14:14.842468Z","iopub.execute_input":"2025-06-12T08:14:14.842804Z","iopub.status.idle":"2025-06-12T08:14:14.860761Z","shell.execute_reply.started":"2025-06-12T08:14:14.842779Z","shell.execute_reply":"2025-06-12T08:14:14.859612Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Summary and Conclusion\n\nIn this notebook, we've built a Logistic Regression model to classify land cover types using NDVI time-series data. Our approach included:\n\n1. **Feature Engineering**: Created statistical features from NDVI time series to capture temporal patterns\n2. **Data Preprocessing**: Handled missing values with median imputation and standardized features\n3. **Model Optimization**: Used GridSearchCV to find optimal hyperparameters\n4. **Model Evaluation**: Achieved high accuracy on the validation set\n5. **Feature Analysis**: Identified the most important features for each land cover class","metadata":{}}]}